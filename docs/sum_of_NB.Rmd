---
title: "sum_of_NB"
author: "Mangiola Stefano"
date: "3/8/2019"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#devtools::load_all()
library(tidyverse)
library(magrittr)
library(rstan)
library(foreach)
library(doParallel)
registerDoParallel()
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

my_theme = 	
	theme_bw() +
	theme(
		panel.border = element_blank(),
		axis.line = element_line(),
		panel.grid.major = element_line(size = 0.2),
		panel.grid.minor = element_line(size = 0.1),
		text = element_text(size=12),
		legend.position="bottom",
		aspect.ratio=1,
		axis.text.x = element_text(angle = 90, hjust = 1),
		strip.background = element_blank(),
		axis.title.x  = element_text(margin = margin(t = 10, r = 10, b = 10, l = 10)),
		axis.title.y  = element_text(margin = margin(t = 10, r = 10, b = 10, l = 10))
	)

source("https://gist.githubusercontent.com/stemangiola/dd3573be22492fc03856cd2c53a755a9/raw/e4ec6a2348efc2f62b88f10b12e70f4c6273a10a/tidy_extensions.R")

```

Testing how well is the distribution of sum of neg. binom. vars with identical overdispersion well approximated by a single neg. binom distribution.

Currently assuming the same `phi` for all variables. 

```{r, echo=FALSE, include=F}
#Function to simulate N draws from the sum of NBs
generate_nb_sum <- function(N, n_means, phi, means, props = rep(1, mean %>% length)) {
  sums = numeric(N)
  for(n in 1:N) {
    sums[n] = sum(rnbinom(n_means, mu = means, size = phi) %>% multiply_by(props) %>% floor)
  }
  list(sums = sums, means = means, phi = phi)
}

#Loading models used to match a distribution
nb_model = stan_model("nb.stan")
```


```{r, cache=TRUE}
 n_means = 2

len <- 1000
my_phi <- numeric(len)
my_phi[1] <- 1
my_phi[2] <- 1
for (i in 3:len) { 
    my_phi[i] <- my_phi[i-1]+my_phi[i-2]
}
my_phi =(my_phi/max(my_phi)*1000) + 0.001

foreach(my_phi=my_phi, .combine=bind_rows) %do% {
	#phi = 0.1

	data = generate_nb_sum(10000, n_means, my_phi, means = MCMCpack::rdirichlet(1, c(100, 200)) * 2000)

	 
  opt_res = optimizing(
  	nb_model, 
  	data = list(N = length(data$sums), counts = data$sums)
  )
  
  tibble(
  	method="approx", mean=opt_res$par["mu"], phi=opt_res$par["phi"]
  ) %>%
  	bind_rows(
  		 tibble(method="theory", mean=sum(data$means), phi=my_phi * (sum(data$means) ^ 2) / sum(data$means ^ 2))
  	) %>% 
  	mutate(real_phi = my_phi)

} %>%
	gather(param, value, c("phi", "mean")) %>%
	ggplot(aes(x=real_phi, y=value, color=method)) + 
	geom_line() + 
	scale_x_log10() + 
	facet_wrap(~param, scale="free")
  
  
```

```{r, cache=TRUE}

foreach(my_tot=seq(1, 1000, 10), .combine=bind_rows) %:%
	foreach(my_phi=c(0.001, 0.01, 0.1, 1), .combine=bind_rows) %do% {
	#phi = 0.1

	data = generate_nb_sum(10000, n_means, my_phi, means = MCMCpack::rdirichlet(1, c(100, 200)) * my_tot)

	 
  opt_res = optimizing(
  	nb_model, 
  	data = list(N = length(data$sums), counts = data$sums),
  	init = list(mu = sum(data$means))
  )
  
  tibble(
  	method="approx", mean=opt_res$par["mu"], phi=opt_res$par["phi"]
  ) %>%
  	bind_rows(
  		 tibble(method="theory", mean=sum(data$means), phi=my_phi * (sum(data$means) ^ 2) / sum(data$means ^ 2))
  	) %>% 
  	mutate(real_tot = my_tot, my_phi = my_phi)

} %>%
	gather(param, value, c("phi", "mean")) %>%
	ggplot(aes(x=real_tot, y=value, color=method)) + 
	geom_line() + 
	scale_x_log10() + 
	facet_wrap(my_phi~param, scale="free")
  
  



```

## Varying sigmas

```{r, cache=TRUE}

foreach(my_tot=seq(1, 1000, 20), .combine=bind_rows) %:%
	foreach(my_phi=c(0.001, 0.01, 0.1, 1), .combine=bind_rows) %do% {

	my_my_phi = c(my_phi, my_phi*5)
	my_mean = MCMCpack::rdirichlet(1, c(100, 200)) * my_tot
		
	data = generate_nb_sum(1000, 2, my_my_phi, means = my_mean)

  opt_res = optimizing(
  	nb_model, 
  	data = list(N = length(data$sums), counts = data$sums),
  	init = list(mu = sum(data$means))
  )

  tibble(
  	method="approx", mean=opt_res$par["mu"], phi=opt_res$par["phi"]
  ) %>%
  	bind_rows(
  		 tibble(
  		 	method=c("theory", "theory generalised"), 
  		 	mean=sum(data$means), 
  		 	phi=c(
  		 		 mean(my_my_phi) * (sum(data$means) ^ 2) / sum(data$means ^ 2),
  		 		(sum(sqrt(my_my_phi) * data$means) ^ 2) / sum(data$means ^ 2)
  		 	)
  		 )
  	) %>% 
  	mutate(real_tot = my_tot, my_phi = my_phi)

} %>%
	gather(param, value, c("phi", "mean")) %>%
	ggplot(aes(x=real_tot, y=value, color=method)) + 
	geom_line() + 
	scale_x_log10() + 
	facet_wrap(my_phi~param, scale="free")



	generate_nb_sum(10000, 2, c(0.1, 1), means = c(100, 500)) %$% 
	sums %>%
	MASS::fitdistr("Negative Binomial") 

1 * (sum(c(1:10*10)) ^ 2) / sum(c(1:10*10) ^ 2)

(sqrt(0.1)*100 + sqrt(1)*500)^2/(100^2+500^2)

```

gamma k/theta

```{r, cache=TRUE}
generate_gamma_sum <- function(N, k, theta) {
	n_means = length(k)
  sums = numeric(N)
  for(n in 1:N) sums[n] = sum(rgamma(n_means, shape = k, scale = theta))
  list(sums = sums, shape = k, scale = theta)
}

k=c(3,4,5, 10)
theta=c(2,2,2, 10)
	generate_gamma_sum(10000, k, theta) %$% 
	sums %>%
	MASS::fitdistr("gamma") %>% 
	unlist %>% {
		c((.)[1], 1/(.)[2])
	}
	
	k_sum = sum(theta*k)^2/sum(theta^2*k)
	theta_sum = sum(k*theta)/k_sum
print(c(k_sum, theta_sum))




```

## gamma shape rate

```{r, cache=TRUE}


shape=c(3,4,5, 10)
rate=c(0.5, 0.5, 0.5, 0.1)
	generate_gamma_sum(10000, shape, rate) %$% 
	sums %>%
	MASS::fitdistr("gamma")
	
	k_sum = sum(shape/rate)^2/sum(shape/(rate^2))
	theta_sum = 1/(sum(shape/rate)/k_sum)
print(c(k_sum, theta_sum))

```

## Integration with read data NB

```{r, cache=TRUE}
# load file
fit_parsed <- read_csv("~/unix3XX/PhD/deconvolution/ARMET/docs/fit_level1_20032019.csv")

#Loading models used to match a distribution
nb_model = stan_model("nb.stan")

sum_nb_through_gamma = function(mu, phi){
	# Conversion to gama
	shape = (mu*phi)/(mu+phi)
	rate = phi/(mu+phi)
	
	# Calculating gamma
	shape_sum = sum(shape/rate)^2/sum(shape/(rate^2))
	rate_sum = 1/(sum(shape/rate)/shape_sum)
	
	# Conversion to NB
	mu_sum = shape_sum/rate_sum
	phi_sum = mu_sum^2 / ( (shape_sum/rate_sum^2) - mu_sum)
	c(mu_sum, phi_sum)
}

foreach(dummy=1:1000=, .combine=bind_rows) %dopar% {

	df = 
		fit_parsed %>% 
		group_by(cut(lambda, breaks=3)) %>%
		sample_n(1) 
	
		data = generate_nb_sum(
			10000, 
			df %>% nrow, 
			df %>% pull(sigma_raw) %>% exp %>% `^` (-1), 
			means = df %>% pull(lambda) %>% exp
		)

	  opt_res = optimizing(
	  	nb_model, 
	  	data = list(N = length(data$sums), counts = data$sums),
	  	init = list(mu = sum(data$means))
	  )  
	  
	 tibble(	
  	method="approx",
  	mean=opt_res$par["mu"],
  	phi=opt_res$par["phi"] 
  ) %>%
	bind_rows({
		theory = sum_nb_through_gamma(
			df %>% pull(lambda) %>% exp, 
			df %>% pull(sigma_raw) %>% exp %>% `^` (-1)
		)
		
		 tibble(
		 	method=c("theory"), 
		 	mean=theory[1], 
		 	phi=theory[2]
		 )
	}) %>%
  mutate(run = dummy)
	 
	
} %>%
	tidyr::gather(param, value, c("phi", "mean")) %>%
	ggplot(aes(x=run, y=value, color=method)) + 
	geom_point(alpha=0.5) + 
	facet_wrap(~param, scale="free") + 
	my_theme

```
## Does phi change when only a proportion on NB is sampled?

yes it does, in case the fraction gets really low. But I will verify whether is a problem for real data. Interestinyl the raltionship is not exponential, otherwise ou inferred lamba/sigma expnential trend cold be easily explain this effect.

```{r, cache=TRUE}

#Loading models used to match a distribution
nb_model = stan_model("nb.stan")
multiplier = 1
foreach(divisor = seq(1, 100, 1), .combine = bind_rows) %:%
	foreach(size = c(1, 10, 100)) %dopar% {
		x = rnbinom(10000, mu = 1000 * multiplier, size = size) %>% 
		    divide_by(divisor * multiplier) %>% round
		#multiplier=2
		tibble(
			size = size,
			divisor = divisor,
			multiplier = multiplier,
			`estimated size` = 
				optimizing(
				    nb_model, 
				    data = list(N = x %>% length, counts = x),
				    init = list(mu = x %>% mean)
				)  %$% par %>% unlist %>% `[[` (2) 
		)
	} %>%
	ggplot(aes(x = divisor, y = `estimated size`, color=factor(size))) + 
	geom_point() + 
	geom_smooth(method="lm", formula= (y ~ exp(x)), se=FALSE) +
	geom_hline(yintercept = size) +
	facet_wrap(~ size, scales = "free") +  
	my_theme
```

## Test whether a more naive but efficient way for calculating sigma exists

Apparently exists

```{r, cache=TRUE}
# load file
fit_parsed <- read_csv("~/unix3XX/PhD/deconvolution/ARMET/docs/fit_level1_20032019.csv")

#Loading models used to match a distribution
nb_model = stan_model("nb.stan")

sum_nb_through_method_of_moment = function(mu, phi){
	
	mean=sum(mu)
 	phi= sum(mu)^2 / sum(mu^2 / phi)
 	
 	c(mean, phi)

}

foreach(dummy=1:1000, .combine=bind_rows) %dopar% {

	df = 
		fit_parsed %>% 
		group_by(cut(lambda, breaks=3)) %>%
		sample_n(1) 
	
		data = generate_nb_sum(
			10000, 
			df %>% nrow, 
			df %>% pull(sigma_raw) %>% exp %>% `^` (-1), 
			means = df %>% pull(lambda) %>% exp
		)

	  opt_res = optimizing(
	  	nb_model, 
	  	data = list(N = length(data$sums), counts = data$sums),
	  	init = list(mu = sum(data$means))
	  )  
	  
	 tibble(	
  	method="approx",
  	mean=opt_res$par["mu"],
  	phi=opt_res$par["phi"] 
  ) %>%
	bind_rows({
		theory = sum_nb_through_method_of_moment(
			df %>% pull(lambda) %>% exp, 
			df %>% pull(sigma_raw) %>% exp %>% `^` (-1)
		)
		
		 tibble(
		 	method=c("theory"), 
		 	mean=theory[1], 
		 	phi=theory[2]
		 )
	}) %>%
  mutate(run = dummy)
	 
	
} %>%
	tidyr::gather(param, value, c("phi", "mean")) %>%
	ggplot(aes(x=run, y=value, color=method)) + 
	geom_point(alpha=0.5) + 
	facet_wrap(~param, scale="free") + 
	my_theme

```

As example, highly transcribed genes, from a really rare population

```{r, cache=TRUE}

divisor = 0.0001
size=10
tibble(
    subsampling = rnbinom(10000, mu = 10000 , size = size) %>% 
    	multiply_by(divisor ) %>% 
    	floor ,
    div = rnbinom(10000, mu = 10000 * divisor, size = size) 
) %>% gather(model, y) %>%
    ggplot(aes(y, color=model)) + geom_density() + my_theme

```

Let's try with real lambda/sigma ratios ad prportions

```{r}


foreach(dummy=1:1000, .combine=bind_rows) %dopar% {

	df = 
		fit_parsed %>% 
		filter(lambda > 5) %>%
		sample_n(3) %>%
		mutate(prop = exp(c(1, 3, 6))/sum(exp(c(1, 3, 6)))) 


		data = generate_nb_sum(
			10000, 
			df %>% nrow, 
			df %>% pull(sigma_raw) %>% exp %>% `^` (-1), 
			means = df %>% pull(lambda) %>% exp,
			props = df %>% pull(prop)
		)

	  opt_res = optimizing(
	  	nb_model, 
	  	data = list(N = length(data$sums), counts = data$sums),
	  	init = list(mu = sum(data$means))
	  )  
	  
	 tibble(	
  	method="approx",
  	mean=opt_res$par["mu"],
  	phi=opt_res$par["phi"] 
  ) %>%
	bind_rows({
		theory = sum_nb_through_gamma(
			df %>% mutate(mean = (lambda %>% exp) * prop ) %>% pull(mean), 
			df %>% pull(sigma_raw) %>% exp %>% `^` (-1)
		)
		
		 tibble(
		 	method=c("theory"), 
		 	mean=theory[1], 
		 	phi=theory[2]
		 )
	}) %>%
  mutate(run = dummy)
	 
	
} %>%
	tidyr::gather(param, value, c("phi", "mean")) %>%
	ggplot(aes(x=run, y=value, color=method)) + 
	geom_point(alpha=0.5) + 
	facet_wrap(~param, scale="free") + 
	my_theme



```

## Now with the efficient calculation

```{r}


foreach(dummy=1:1000, .combine=bind_rows) %dopar% {

	df = 
		fit_parsed %>% 
		{
			bind_rows(
				(.) %>% filter(lambda > 5) %>% sample_n(1),
				(.) %>% filter(lambda < 5) %>% sample_n(2)
			)
		} %>%
		sample_n(3) %>%
		mutate(prop = exp(c(1, 3, 6))/sum(exp(c(1, 3, 6)))) 


		data = generate_nb_sum(
			10000, 
			df %>% nrow, 
			df %>% pull(sigma_raw) %>% exp %>% `^` (-1), 
			means = df %>% pull(lambda) %>% exp,
			props = df %>% pull(prop)
		)

	  opt_res = optimizing(
	  	nb_model, 
	  	data = list(N = length(data$sums), counts = data$sums),
	  	init = list(mu = sum(data$means))
	  )  
	  
	 tibble(	
  	method="approx",
  	mean=opt_res$par["mu"],
  	phi=opt_res$par["phi"] 
  ) %>%
	bind_rows({
		theory = sum_nb_through_method_of_moment(
			df %>% mutate(mean = (lambda %>% exp) * prop ) %>% pull(mean), 
			df %>% pull(sigma_raw) %>% exp %>% `^` (-1)
		)
		
		 tibble(
		 	method=c("theory"), 
		 	mean=theory[1], 
		 	phi=theory[2]
		 )
	}) %>%
  mutate(run = dummy)
	 
	
} %>%
	tidyr::gather(param, value, c("phi", "mean")) %>%
	ggplot(aes(x=run, y=value, color=method)) + 
	geom_point(alpha=0.5) + 
	scale_y_log10() +
	facet_wrap(~param, scale="free") + 
	my_theme



```

## Setup linear algebra version of sigma calculation. The two formulations match

```{r}
sum_nb_through_method_of_moment = function(mu, phi){
	
	mean=sum(mu)
 	phi= sum(mu)^2 / sum(mu^2 / phi)
 	
 	c(mean, phi)

}

my_df = 
	foreach(sample=1:10, .combine=bind_rows) %do% {

		df = 
			fit_parsed %>% 
			filter(lambda > 5) %>%
			inner_join( 
				(.) %>%
					distinct(sample) %>%
					slice(c(3, 30, 300)) %>% 
					mutate(ct = 1:3) %>%
					mutate(prop = exp(c(1, 3, 6))/sum(exp(c(1, 3, 6))))
			) %>%
			mutate(sample = !!sample) 
	
	}

prop = my_df %>% distinct(sample, ct, prop) %>% spread(sample, prop) %>% as_matrix(rownames = "ct")
mu = my_df %>% mutate(mean = lambda %>% exp ) %>% distinct(ct, symbol, mean) %>% spread(ct, mean) %>% drop_na() %>% as_matrix(rownames = "symbol")
sigma = my_df %>% mutate(sigma = sigma_raw %>% exp %>% `^` (-1) ) %>% distinct(ct, symbol, sigma) %>% spread(ct, sigma) %>% drop_na() %>% as_matrix(rownames = "symbol")

sums1 = my_df %>% 
	group_by(sample, symbol) %>%
	do({
		theory = sum_nb_through_method_of_moment(
			(.) %>% mutate(mean = (lambda %>% exp) * prop ) %>% pull(mean), 
			(.) %>% pull(sigma_raw) %>% exp %>% `^` (-1)
		)
		
		(.) %>%
			summarise(
				mean_sum=theory[1], 
		 		phi_sum=theory[2]
			)
	}) %>%
	ungroup

sums2 = 
	left_join(
		(mu %*% prop) %>% as_tibble(rownames = "symbol") %>% gather(sample, mean_sum, -symbol),
		mean_sum^2 / ((mu^2 / sigma) %*% prop^2)  %>% as_tibble(rownames = "symbol") %>% gather(sample, sigma_sum, -symbol)
	)

```
